{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import PCIData\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "import vectorbt as vbt\n",
    "import pickle\n",
    "\n",
    "\n",
    "class DataTable:\n",
    "    def __init__(self, pairName:list, constituents:list):\n",
    "        self.pairName = pairName\n",
    "        self.constituents = constituents\n",
    "        self.dict_data={}\n",
    "\n",
    "    def getData(self):\n",
    "        if self.dict_data == {}:\n",
    "            self.setData()\n",
    "        else:\n",
    "            pass\n",
    "        return self.dict_data\n",
    "    \n",
    "    def setData(self):\n",
    "        self.dict_data = dict(zip(self.pairName, self.constituents))\n",
    "        return self.dict_data\n",
    "\n",
    "class Definition:\n",
    "    def f_load_adjPrice(filepath) -> pd.DataFrame:\n",
    "        \"\"\"Load pandas dataframe of the price series\n",
    "        Args:\n",
    "          filepath (str): Filepath to the adjPrice_nasdaq100.csv\"\"\"\n",
    "        try:\n",
    "            adjPrice = pd.read_pickle(filepath)\n",
    "            adjPrice.index = adjPrice[\"date\"]\n",
    "        except Exception as e:\n",
    "            print(\"Error : {e}\")\n",
    "            adjPrice = None\n",
    "        return adjPrice\n",
    "    \n",
    "    def f_selectTicker(tickerList, adjPrice)-> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Select the columns that are in tickerList\n",
    "\n",
    "        Args:\n",
    "            tickerList (list): List of all tickers to take into considerations\n",
    "            adjPrice (pd.DataFrame): Adjusted price of all tickers\n",
    "        \"\"\"\n",
    "        \n",
    "        return adjPrice.loc[:, adjPrice.columns.isin(tickerList)]\n",
    "\n",
    "    def f_insample_outsample(data, year):\n",
    "        data.index = pd.to_datetime(data.index)\n",
    "        \n",
    "        start_of_year = pd.to_datetime(str(year))\n",
    "        \n",
    "        end_of_period = start_of_year + pd.DateOffset(months=6)  # This is \"year + 6 months\"\n",
    "        start_of_period = start_of_year - pd.DateOffset(years=4) \n",
    "        \n",
    "        # Adjusted filter condition using the corrected datetime offsets\n",
    "        all_sample = data[(data.index < end_of_period) & (data.index >= start_of_period)]\n",
    "        \n",
    "        # Cleaning operations\n",
    "        tmp = all_sample.dropna(how=\"all\")\n",
    "        all_sample_cleaned = tmp.dropna(axis=1, how='any')\n",
    "        \n",
    "        \n",
    "        # Select in-sample data: data from the four years prior to the specified year.\n",
    "        # It checks if the year in the data is less than the specified year\n",
    "        # and greater than or equal to four years before the specified year.\n",
    "        \n",
    "        in_sample = all_sample_cleaned[(all_sample_cleaned.index < start_of_year) & (all_sample_cleaned.index >= start_of_period)]\n",
    "        \n",
    "        # Select out-sample data: data from the specified year but only for the first six months.\n",
    "        # It checks if the year in the data is exactly the specified year and\n",
    "        # if the month is less than or equal to 6 (January to June).\n",
    "        out_sample = all_sample_cleaned[(all_sample_cleaned.index >= start_of_year) & (all_sample_cleaned.index <= end_of_period)]\n",
    "\n",
    "        # Return both the in-sample and out-sample datasets.\n",
    "        return in_sample, out_sample\n",
    "\n",
    "\n",
    "    def pairDatabase(consituentList : list) -> pd.DataFrame:\n",
    "    \n",
    "        combinations = [(a, b) for a, b in permutations(consituentList, 2)]\n",
    "\n",
    "        # Create a DataFrame\n",
    "        pairDatabase = pd.DataFrame(combinations, columns=[\"Stock 1\", \"Stock 2\"])\n",
    "        pairDatabase[\"pair\"] = pairDatabase[\"Stock 1\"] + \"/\" + pairDatabase[\"Stock 2\"]\n",
    "        \n",
    "        return pairDatabase\n",
    "\n",
    "class Computation:\n",
    "    def __init__(self, X1, X2) -> None:\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "\n",
    "    def Rsquare(sigmaM, sigmaR, rho):\n",
    "        return 2 * (sigmaM ** 2) / (2 * (sigmaM ** 2) + (1+rho) * sigmaR ** 2)\n",
    "   \n",
    "    def fit_mle(self, W, tol=0.001):\n",
    "        '''\n",
    "        Maximum likelihood estimation of the associated Kalman filter\n",
    "\n",
    "        Parameters:\n",
    "        W (numpy.ndarray): the time-series which we want to model has both permanent and transient components\n",
    "\n",
    "        Returns:\n",
    "        rho (float): estimated value of rho\n",
    "        sigma_M (float): estimated value of sigma_M\n",
    "        sigma_R (float): estimated value of sigma_R\n",
    "        '''\n",
    "\n",
    "        # Set empty list for estimate and ll values\n",
    "        estimates = []\n",
    "        lls = []\n",
    "\n",
    "        # Set distributions for random guesses\n",
    "        rnd_rho = stats.uniform(loc=-1, scale=2)\n",
    "        std = np.std(np.diff(W)) # Compute standard deviation of \"pci process\" (or residuals)\n",
    "        rnd_sigma = stats.norm(loc=std, scale=std / 2) # Why scaled to std / 2? Can we find better? ####################\n",
    "\n",
    "        # Define function for generating random initial guesses from above distributions\n",
    "        def gen_x_i():\n",
    "            return rnd_rho.rvs(), rnd_sigma.rvs(), rnd_sigma.rvs()\n",
    "\n",
    "        # Minimize the negative log-likelihood function\n",
    "        bounds = ((-1, 1), (0, np.inf), (0, np.inf)) # Set bounds for opt.\n",
    "        x_i = self.lagvar_estimate_par(W) # Get initial guesses using lagged variance equations\n",
    "        res = opt.minimize(self.f_to_min_par , x0=(x_i), args=(W), bounds=bounds, tol=tol) # Minimize -ll function\n",
    "\n",
    "        if res.success: # If optimization is a success\n",
    "            estimates.append(res.x) # Save estimates in list\n",
    "            lls.append(-res.fun)  # Save log-likelihood in list\n",
    "        # return res.x\n",
    "    \n",
    "        # Repeat optimization with different random initial values\n",
    "    \n",
    "        n_att = 0\n",
    "        while len(lls) < 10 and n_att < 100:\n",
    "            n_att += 1\n",
    "            x_i = gen_x_i()\n",
    "            res = opt.minimize(self.f_to_min_par, x0=(x_i), args=(W), bounds=bounds, tol=tol)\n",
    "\n",
    "            if res.success:  # If optimization is a success\n",
    "                estimates.append(res.x)  # Save estimates\n",
    "                lls.append(-res.fun)  # Save log-likelihood\n",
    "\n",
    "        try:\n",
    "            argmax = np.argmax(lls)  # Find index of max likelihood\n",
    "            return estimates[argmax] # Return estimates linked to max likelihood\n",
    "        except:\n",
    "            # print('Estimation failed!')\n",
    "            return len(x0) * [np.nan]  # Returns nanns\n",
    "        \n",
    "\n",
    "    def lagvar_estimate_par(self, W):\n",
    "        '''\n",
    "        Estimate parameters of partial AR model using lagged variances. Used for first estimation of parameters\n",
    "\n",
    "        Parameters\n",
    "        W (numpy.ndarray): A partial autoregressive time series\n",
    "\n",
    "        Returns:\n",
    "        rho_lv (float): estimated value for rho\n",
    "        sigma_M_lv (float): estimated value for sigma_M\n",
    "        sigma_R_lv (float): estimated value for sigma_R\n",
    "        '''\n",
    "\n",
    "        # See Matthew Clegg: \"Modeling Time Series with both Permanent and Transient\n",
    "        # Component using the Partially Autoregressive Model\". See equations on page 5.\n",
    "\n",
    "        # Calculate variance of the lagged differences. Left hand side of equation (3)\n",
    "        v1 = np.var(W[1:] - W[:-1])\n",
    "        v2 = np.var(W[2:] - W[:-2])\n",
    "        v3 = np.var(W[3:] - W[:-3])\n",
    "\n",
    "        # Calculate rho from v1, v2, v3. Equations (4), page 5\n",
    "        rho_lv = -(v1 - 2 * v2 + v3) / (2 * v1 - v2)\n",
    "\n",
    "        # Calculate sigma_M. Equations (4), page 5\n",
    "        if (rho_lv + 1) / (rho_lv - 1) * (v2 - 2 * v1) > 0:\n",
    "            sigma_M_lv = np.sqrt(1 / 2 * (rho_lv + 1) / (rho_lv - 1) * (v2 - 2 * v1))\n",
    "        else:\n",
    "            sigma_M_lv = 0\n",
    "\n",
    "        # Calculate sigma_M. Equations (4), page 5\n",
    "        if v2 > 2 * sigma_M_lv ** 2:\n",
    "            sigma_R_lv = np.sqrt(1 / 2 * (v2 - 2 * sigma_M_lv ** 2))\n",
    "        else:\n",
    "            sigma_R_lv = 0\n",
    "\n",
    "        return rho_lv, sigma_M_lv, sigma_R_lv\n",
    "\n",
    "    def fit_ols_on_diff(self):\n",
    "        '''\n",
    "        Fits an OLS model on the first differences of time series X1 and X2\n",
    "\n",
    "        Parameters:\n",
    "        X1 (numpy.ndarray): price time-series\n",
    "        X2 (numpy.ndarray): price time-series\n",
    "\n",
    "        Returns:\n",
    "        results.params[0]: returns the Beta value of our OLS fit\n",
    "        '''\n",
    "        ret_X1 = np.diff(self.X1)\n",
    "        ret_X2 = np.diff(self.X2)\n",
    "\n",
    "        results = sm.OLS(ret_X2, ret_X1).fit()\n",
    "\n",
    "        return results.params[0]\n",
    "\n",
    "    def kalman_estimate(self, W, rho, sigma_M, sigma_R):\n",
    "        '''\n",
    "        Calculate estimates of mean-reverting series (M_t) and random walk series (R_t).\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy.ndarray): A partial autoregressive time-series\n",
    "        rho (float): AR(1) coefficient / mean reversion coefficient.\n",
    "        sigma_M (float): standard deviation of the innovations of the mean-reverting component\n",
    "        sigma_R (float): standard deviation of the innovations of the random walk component\n",
    "\n",
    "        Returns:\n",
    "        M (numpy.ndarray): An estimate of the mean reverting component of our time series\n",
    "        R (numpy.ndarray): An estimate of the random walk component of our time series\n",
    "        eps (numpy.ndarray): Prediction errors for each time step\n",
    "        '''\n",
    "\n",
    "        # See Matthew Clegg: \"Modeling Time Series with both Permanent and Transient\n",
    "        # Component using the Partially Autoregressive Model\". See algo on page 9.\n",
    "\n",
    "        # Create arrays for storing both components and prediction errors\n",
    "        M = np.zeros(len(W))\n",
    "        R = np.zeros(len(W))\n",
    "        eps = np.zeros(len(W))\n",
    "\n",
    "        # Set state at t=0\n",
    "        if sigma_R == 0: # If series is purely mean-reverting\n",
    "            M[0] = W[0]\n",
    "            R[0] = 0\n",
    "        else:\n",
    "            M[0] = 0\n",
    "            R[0] = W[0]\n",
    "\n",
    "        # Calculate Kalman gains\n",
    "        if sigma_M == 0: # If series is purely a random walk\n",
    "            K_M = 0\n",
    "            K_R = 1\n",
    "        elif sigma_R == 0: # If series is purely mean-reverting\n",
    "            K_M = 1\n",
    "            K_R = 0\n",
    "        else:\n",
    "            # See equations (11), page 8\n",
    "            sqr = np.sqrt((1 + rho) ** 2 * sigma_R ** 2 + 4 * sigma_M ** 2)\n",
    "            K_M = 2 * sigma_M ** 2 / (sigma_R * (sqr + rho * sigma_R + sigma_R) + 2 * sigma_M ** 2) # Compute gain for M\n",
    "            K_R = 2 * sigma_R / (sqr - rho * sigma_R + sigma_R) # Compute gain for R\n",
    "\n",
    "        # Calculate estimates recursively\n",
    "        for i in range(1, len(W)):\n",
    "            w_hat = rho * M[i - 1] + R[i - 1] # Predicted value of W[i]\n",
    "            eps[i] = W[i] - w_hat # Prediction error\n",
    "            M[i] = rho * M[i - 1] + eps[i] * K_M\n",
    "            R[i] = R[i - 1] + eps[i] * K_R\n",
    "\n",
    "        return M, R, eps\n",
    "\n",
    "    def calc_log_like(self, W, rho, sigma_M, sigma_R):\n",
    "        '''\n",
    "        Compute negative log likelihood function\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy.ndarray): A partial autoregressive time series\n",
    "        rho (float): AR(1) coefficient / mean reversion coefficient.\n",
    "        sigma_M (float): standard deviation of the innovations of the mean-reverting component\n",
    "        sigma_R (float): standard deviation of the innovations of the random walk component\n",
    "\n",
    "        Returns:\n",
    "        ll (float): Value of the log likelihood, a measure of goodness of fit for our model\n",
    "        '''\n",
    "\n",
    "        N = len(W)\n",
    "        _, _, eps = self.kalman_estimate(W, rho, sigma_M, sigma_R) # Compute the prediction errors for each time step\n",
    "\n",
    "        # Compute the value of the log-likelihood function\n",
    "        ll = -(N - 1) / 2 * np.log(2 * np.pi * (sigma_M ** 2 + sigma_R ** 2)) - 1 / (\n",
    "                2 * (sigma_M ** 2 + sigma_R ** 2)) * np.sum(eps[1:] ** 2)\n",
    "\n",
    "        return ll\n",
    "\n",
    "    def f_to_min_par(self, x_i, W):\n",
    "        rho, sigma_M, sigma_R = x_i\n",
    "        '''\n",
    "        Define the function to minimize for PAR model\n",
    "        '''\n",
    "        return -self.calc_log_like(W, rho, sigma_M, sigma_R)\n",
    "\n",
    "    def f_to_min_pci(self, x_i, X1, X2):\n",
    "        '''\n",
    "        Define function to minimize\n",
    "        '''\n",
    "        if len(x_i) == 5 :\n",
    "            alpha, beta, rho, sigma_M, sigma_R = x_i\n",
    "            \n",
    "        elif len(x_i) == 3:\n",
    "            alpha, beta, sigma_R = x_i\n",
    "            rho = 0\n",
    "            sigma_M = 0\n",
    "        \n",
    "        W = X2 - beta * X1 - alpha\n",
    "        return -self.calc_log_like(W, rho, sigma_M, sigma_R)\n",
    "\n",
    "    def fit_pci(self, tol=0.001, LRT_mode: bool = True):\n",
    "        '''\n",
    "        Fit partial cointegrated model to time series X1 and X2 such that:\n",
    "            - X_2,t = alpha + beta * X_1,t + W_t\n",
    "            - W_t = M_t + R_t\n",
    "            - M_t = rho * M_t-1 + eps(M_t)\n",
    "            - R_t = R_t-1 + eps(R_t)\n",
    "            - eps(M_t) ∼ NID(0, sigma_M)\n",
    "            - eps(R_t) ∼ NID(0, sigma_R)\n",
    "\n",
    "        Parameters:\n",
    "        X1 (numpy.ndarray): time series\n",
    "        X2 (numpy.ndarray): time series, supposedly partially cointegrated with X1\n",
    "\n",
    "        Returns:\n",
    "        alpha (float): estimated value for alpha (linear regression parameter)\n",
    "        beta (float): estimated value for beta (linear regression parameter)\n",
    "        rho (float): estimated AR(1) coefficient / mean reversion coefficient.\n",
    "        sigma_M (float): standard deviation of the innovations of the mean-reverting component\n",
    "        sigma_R (float): standard deviation of the innovations of the random walk component\n",
    "        '''\n",
    "\n",
    "        # Calculate initial guess for beta with linear regression\n",
    "        results = self.fit_ols_on_diff()\n",
    "        beta_i = results\n",
    "\n",
    "        # Calculate initial guess for alpha\n",
    "        alpha_i = self.X2[0] - beta_i * self.X1[0]\n",
    "\n",
    "        # Calculate residuals W and initial guesses for rho, sigma_M, sigma_R (params_i)\n",
    "        W = self.X2 - alpha_i - beta_i * self.X1\n",
    "        params_i = self.fit_mle(W)\n",
    "\n",
    "        rho, sigma_M, sigma_R = params_i\n",
    "\n",
    "        # perform optimization depending on the mode (Complete Model)\n",
    "        x_i = (alpha_i, beta_i, rho, sigma_M, sigma_R)  # initial guess\n",
    "        bounds = [(None, None), (None, None), (-1, 1), (0.0001, None), (0.0001, None)]\n",
    "        res = opt.minimize(self.f_to_min_pci, x_i, args=(self.X1, self.X2), tol=tol, bounds=bounds)\n",
    "        alpha, beta, rho, sigma_M, sigma_R = res.x\n",
    "        ll_model = -res.fun\n",
    "\n",
    "        # W = self.X2 - alpha - beta * self.X1\n",
    "\n",
    "        if LRT_mode:\n",
    "            # perform optimization for the random Walk H0)\n",
    "            # perform optimization depending on the mode (Complete Model)\n",
    "            x_i = (alpha_i, beta_i, sigma_R)  # initial guess\n",
    "            bounds = [(None, None), (None, None), (0.0001, None)]\n",
    "            res = opt.minimize(self.f_to_min_pci, x_i, args=(self.X1, self.X2), tol=tol, bounds=bounds)\n",
    "            ll_randomWalk = -res.fun\n",
    "            lrt = ll_randomWalk - ll_model\n",
    "            return alpha, beta, rho, sigma_M, sigma_R, ll_model, ll_randomWalk, lrt\n",
    "\n",
    "        return alpha, beta, rho, sigma_M, sigma_R, ll_model\n",
    "    \n",
    "    def f_get_elligibility(Stock1, Stock2, inSample):\n",
    "        \n",
    "        X = inSample[[Stock1, Stock2]].dropna()\n",
    "\n",
    "        X1 = X[Stock1]\n",
    "        X2 = X[Stock2]\n",
    "\n",
    "        coint = Computation(X1, X2)\n",
    "        alpha_hat, beta_hat, rho_hat, sigma_M_hat, sigma_R_hat, ll_model, ll_randomWalk, ll_ratio = coint.fit_pci(LRT_mode = True)\n",
    "        # M_hat, R_hat, _ = coint.kalman_estimate(W_hat, rho_hat, sigma_M_hat, sigma_R_hat)\n",
    "\n",
    "        Rsquare_res = Computation.Rsquare(sigma_M_hat, sigma_R_hat, rho_hat)\n",
    "\n",
    "        # print([ll_randomWalk, ll_model])\n",
    "        \n",
    "        # for pair in pairName:\n",
    "\n",
    "        return {\"elligibility\":[rho_hat, Rsquare_res, ll_ratio],\n",
    "            \"estimators\" :[alpha_hat, beta_hat, rho_hat, sigma_M_hat, sigma_R_hat]}\n",
    "    \n",
    "    def f_compute_estimate(pairName: pd.DataFrame, inSample: pd.DataFrame, pairData: pd.DataFrame) -> tuple :\n",
    "        N = len(pairName)\n",
    "        elligibilityData = np.zeros((N, 3))\n",
    "        estimationData = np.zeros((N, 5))\n",
    "        \n",
    "        \n",
    "\n",
    "        for pair in tqdm(range(N), desc=\"Processing Pairs\"):\n",
    "            try:\n",
    "                pairName_i = pairName[pair]\n",
    "                # print(f\"Doing : {pairName_i}\")\n",
    "                Stock1 = pairData[\"Stock 1\"][pair]\n",
    "                Stock2 = pairData[\"Stock 2\"][pair]\n",
    "                \n",
    "                dict_res = Computation.f_get_elligibility(Stock1, Stock2, inSample)  # Call f_get_elligibility using Computation class\n",
    "                elligibilityData[pair] = dict_res[\"elligibility\"]\n",
    "                # latentVariablesData[pairName[pair]] = dict_res[\"latentVariables\"]\n",
    "                estimationData[pair] = dict_res[\"estimators\"]\n",
    "                #print(f\"{pairName_i}: Done\")\n",
    "            except Exception as e: \n",
    "                print(f\"Error with {pairName_i} and {e}\")\n",
    "        return elligibilityData, estimationData\n",
    "\n",
    "\n",
    "    def f_create_dataframe(elligibilityData: pd.DataFrame, estimationData: pd.DataFrame, pairName: pd.DataFrame)-> tuple:\n",
    "    \n",
    "        elligibilityData = pd.DataFrame(elligibilityData, index = pairName, columns=[\"rho\", \"Rsquare\", \"ll_ratio\"])\n",
    "        elligibilityData[\"ll_ratio\"].fillna(0)\n",
    "        elligibility_sorted_data = elligibilityData[\n",
    "                                                    (elligibilityData[\"rho\"] > 0.5) & \n",
    "                                                     (elligibilityData[\"Rsquare\"] > 0.5)\n",
    "                                                ].sort_values(by=\"ll_ratio\", ascending=True)\n",
    "        # Dataframe des paramètres estimés\n",
    "        estimatorData = pd.DataFrame(estimationData, index = pairName, columns=[\"alpha_hat\", \"beta_hat\", \"rho_hat\", \"sigma_M_hat\", \"sigma_R_hat\"])\n",
    "        \n",
    "        return elligibility_sorted_data, estimatorData\n",
    "    \n",
    "    def f_pairSelected(elligibility_sorted_data, n_keep :int):\n",
    "        # Nombre de pair elligible\n",
    "        n_pair = len(elligibility_sorted_data)\n",
    "        if n_pair == 0:\n",
    "                print(\"Elligibility_sorted_data is empty\")\n",
    "                return None\n",
    "        \n",
    "        n_elligible = len(elligibility_sorted_data.index)\n",
    "        \n",
    "        n = max(1, min(n_pair, n_keep))\n",
    "        \n",
    "        pairListSelected = elligibility_sorted_data.index[:n].to_list()\n",
    "\n",
    "        return pairListSelected\n",
    "    \n",
    "    def get_stock_list(pairData, col):\n",
    "        selected_pairs = pairData.loc[pairData[\"pair\"].isin(col)]\n",
    "        tmp = selected_pairs[[\"Stock 1\", \"Stock 2\"]].values.flatten().tolist()\n",
    "        stock_list = list(set(tmp))\n",
    "        return stock_list\n",
    "    \n",
    "    def get_stock1_stock2(pair, pairData):\n",
    "        S1, S2 = pairData.loc[pairData[\"pair\"] == pair, [\"Stock 1\", \"Stock 2\"]].iloc[0].values\n",
    "        return S1, S2\n",
    "    \n",
    "    def get_stock_price(df_price, stockList):\n",
    "        return df_price[stockList].iloc[:, 0], df_price[stockList].iloc[:, 1]\n",
    "    \n",
    "    \n",
    "    def f_create_Mt(pairListSelected, estimatorData, pairData, stock_price):\n",
    "        \n",
    "        estimator_Data_selected = estimatorData.loc[estimatorData.index.isin(pairListSelected)]\n",
    "        \n",
    "        n = len(pairListSelected)\n",
    "        df_mt = np.zeros((len(stock_price), n))\n",
    "        df_hedge = np.zeros((len(stock_price), n))\n",
    "\n",
    "        for i in range(n):\n",
    "            pair_tmp = pairListSelected[i]\n",
    "            \n",
    "            S1, S2 = Computation.get_stock1_stock2(pair_tmp, pairData)\n",
    "\n",
    "            X1, X2 = Computation.get_stock_price(stock_price, [S1, S2])\n",
    "\n",
    "            # Retrieve estimater\n",
    "            alpha, beta, rho, sigmaM, sigmaR = estimator_Data_selected.loc[pair_tmp].to_list()\n",
    "\n",
    "            W = X2 - alpha - beta * X1\n",
    "            coint_tmp = Computation(X1, X2)\n",
    "            _, Mt, _ = coint_tmp.kalman_estimate(W, rho, sigmaM, sigmaR)\n",
    "            df_mt[:, i] = Mt\n",
    "            # Number of share invested in S1 for 1$ invested in X2\n",
    "            df_hedge[:, i] = beta / X2 # The amount of share\n",
    "            \n",
    "        return pd.DataFrame(data = df_mt, columns=pairListSelected), pd.DataFrame(data = df_hedge, columns = pairListSelected)\n",
    "      \n",
    "    \n",
    "    def rolling_z_score(series: pd.Series, window: int) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Calculate the rolling Z-score of a given time series.\n",
    "        \n",
    "        Parameters:\n",
    "        series (pd.Series): The time series data.\n",
    "        window (int): The size of the rolling window.\n",
    "        \n",
    "        Returns:\n",
    "        pd.Series: A series of rolling Z-scores.\n",
    "        \"\"\"\n",
    "        # Convert numpy array to pandas Series if necessary\n",
    "        # Convert numpy array to pandas Series if necessary\n",
    "    # Initialize an array to hold the rolling Z-scores\n",
    "        z_scores = np.full(series.shape, np.nan)\n",
    "\n",
    "        # Loop through the series to calculate the rolling mean and standard deviation\n",
    "        for i in range(window - 1, len(series)):\n",
    "            window_values = series[i - window + 1 : i+1]\n",
    "            #print([window_values[-1], series[i]])\n",
    "            window_mean = np.mean(window_values)\n",
    "            window_std = np.std(window_values, ddof=1)\n",
    "\n",
    "            # Avoid division by zero\n",
    "            if window_std != 0:\n",
    "                z_scores[i] = (series.iloc[i] - window_mean) / window_std\n",
    "\n",
    "        return z_scores\n",
    "    \n",
    "    def percentile_zscore(value, data):\n",
    "        # Calculate the percentile-based z-score for a single value\n",
    "        \n",
    "        percentile = np.sum(data < value) / (len(data) - 1)  # Calculate percentile\n",
    "        zscore = percentile * 2 - 1   # Transform to [-1, 1] range\n",
    "        return zscore\n",
    "    \n",
    "    \n",
    "    def calculate_zscores_historic(historical_data, initial_in_sample_size = 2):\n",
    "        # Traiter nan values\n",
    "        z = historical_data.copy()\n",
    "        mask = historical_data.isna()\n",
    "        non_nan_values_sample = historical_data[~mask]\n",
    "        \n",
    "        # Ensure there's enough historical data for the initial in-sample calculation\n",
    "        if len(non_nan_values_sample) < initial_in_sample_size :\n",
    "            raise ValueError(\"Not enough historical data for the specified initial in-sample size.\")\n",
    "        \n",
    "        # Calculate in-sample z-scores for the initial baseline\n",
    "        \n",
    "        sample = list(non_nan_values_sample[:initial_in_sample_size])\n",
    "        out_sample_data = list(non_nan_values_sample[initial_in_sample_size:])\n",
    "\n",
    "        zscores = [Computation.percentile_zscore(x, np.array(sample)) for x in sample]\n",
    "        \n",
    "        # Calculate out-of-sample z-scores for any additional data\n",
    "        for value in out_sample_data:\n",
    "            # Update in-sample data with each new data point for out-of-sample calculation\n",
    "            sample.append(value)\n",
    "            # Calculate z-score for the new data point\n",
    "            zscores.append(Computation.percentile_zscore(value, np.array(sample)))\n",
    "        \n",
    "        z[~mask] = zscores\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    \n",
    "    \n",
    "class vbt_strategyPCI:\n",
    "        \n",
    "    def f_pre_vectorBT(pairListSelected, estimatorData, pairData, inSample ,outSample) -> dict:\n",
    "        \n",
    "        Mt_inSample, hedgeS1_inSample = Computation.f_create_Mt(pairListSelected, estimatorData, pairData, inSample)\n",
    "        Mt_outSample, hedgeS1_outSample = Computation.f_create_Mt(pairListSelected, estimatorData, pairData, outSample)\n",
    "        \n",
    "        stockList = Computation.get_stock_list(pairData, pairListSelected)\n",
    "        \n",
    "        dict_output = { \"Mt\":       {\"inSample\" : Mt_inSample,\n",
    "                                    \"outSample\" : Mt_outSample},\n",
    "                        \"hedgeS1\": {\"inSample\" : hedgeS1_inSample,\n",
    "                                    \"outSample\": hedgeS1_outSample},\n",
    "                        \"Price\": {\"inSample\": inSample[stockList],\n",
    "                                \"outSample\": outSample[stockList]}}\n",
    "        \n",
    "        return dict_output\n",
    "\n",
    "\n",
    "    def custom_indicator(Mt : np.array, window: int = 20):\n",
    "        \"\"\"\n",
    "        La fonction qui sert comme indicateur sur mesure pour vectorBT \n",
    "        \"\"\"\n",
    "        \n",
    "        Mt_df = pd.DataFrame(Mt)\n",
    "        \n",
    "        # Mt_zscore = Mt_df.apply(func=lambda x: Computation.calculate_zscores_historic(x, initial_in_sample_size=2), axis=0) # Normalisé entre -1 et 1\n",
    "        Mt_zscore = Mt_df.apply(func=lambda x: Computation.rolling_z_score(x, window), axis=0) # Normalisé \n",
    "        return Mt_zscore\n",
    "\n",
    "\n",
    "    def f_get_Mt_normalize(Mt : dict, window: int) -> vbt.indicators.factory:\n",
    "\n",
    "        strategyBase = vbt.IndicatorFactory(\n",
    "                                                class_name = \"StrategyBase\",\n",
    "                                                short_name = \"StratBase\",\n",
    "                                                input_names = [\"Mt\"],\n",
    "                                                param_names= [\"window\"],\n",
    "                                                output_names = [\"z_score\"],\n",
    "                                                ).from_apply_func(\n",
    "                                                                vbt_strategyPCI.custom_indicator,\n",
    "                                                                window = window\n",
    "                                                                )\n",
    "        strategy = strategyBase.run(Mt)\n",
    "        return strategy\n",
    "\n",
    "\n",
    "\n",
    "    def f_get_signals(strategy: vbt.indicators.factory , entry_threshold: float, exit_threshold: float) -> tuple:\n",
    "        \n",
    "        # Generate entry signals (Z-score crosses above the entry threshold)\n",
    "        upper_crossed = strategy.z_score.vbt.crossed_above(entry_threshold)\n",
    "\n",
    "        # Generate exit signals (Z-score crosses below the exit threshold)\n",
    "        lower_crossed = strategy.z_score.vbt.crossed_below(exit_threshold)\n",
    "\n",
    "        clean_upper_crossed, clean_lower_crossed = upper_crossed.vbt.signals.clean(lower_crossed)  \n",
    "        \n",
    "        return (clean_upper_crossed, clean_lower_crossed)\n",
    "\n",
    "\n",
    "    def f_backtest(dict_pre_backtest : dict, type_back : str, hedgeS1: pd.DataFrame, price: pd.DataFrame, pairBacktested: str, pairData: pd.DataFrame) -> vbt.portfolio.base.Portfolio:\n",
    "\n",
    "            \n",
    "            clean_upper_crossed = dict_pre_backtest[type_back][\"clean_upper_crossed\"]\n",
    "            clean_lower_crossed = dict_pre_backtest[type_back][\"clean_lower_crossed\"]\n",
    "            \n",
    "            positions_fill = price.copy()\n",
    "            positions_fill.iloc[:, :] = False\n",
    "\n",
    "            # Now, for sizing, long_entries, etc., we need independent copies of positions_fill\n",
    "            sizing = positions_fill.copy()\n",
    "            sizing.iloc[:, :] = 0\n",
    "            long_entries = positions_fill.copy()\n",
    "            short_entries = positions_fill.copy()\n",
    "            short_exits = positions_fill.copy()\n",
    "            long_exits = positions_fill.copy()\n",
    "\n",
    "            # Directly access the first column name (pair)\n",
    "\n",
    "            S1, S2 = Computation.get_stock1_stock2(pairBacktested, pairData)\n",
    "\n",
    "            # Les prix\n",
    "            X1, X2 = Computation.get_stock_price(price, [S1, S2])\n",
    "\n",
    "            s1_sizing =  hedgeS1.loc[:, pairBacktested]\n",
    "\n",
    "            s2_sizing = 1 / X2\n",
    "\n",
    "            sizing.loc[clean_upper_crossed[pairBacktested].values, S2] =  s2_sizing.values[clean_upper_crossed[pairBacktested]]\n",
    "\n",
    "            sizing.loc[clean_upper_crossed[pairBacktested].values, S1] = s1_sizing.values[clean_upper_crossed[pairBacktested].values]\n",
    "\n",
    "            short_entries.loc[clean_upper_crossed[pairBacktested].values, S2] = True\n",
    "            long_entries.loc[clean_upper_crossed[pairBacktested].values, S1] = True\n",
    "\n",
    "            short_exits.loc[clean_lower_crossed[pairBacktested].values, S2] = True\n",
    "            long_exits.loc[clean_lower_crossed[pairBacktested].values, S1] = True\n",
    "\n",
    "\n",
    "            pf = vbt.Portfolio.from_signals(\n",
    "                price,\n",
    "                entries = long_entries,\n",
    "                exits = long_exits,\n",
    "                short_entries = short_entries,\n",
    "                short_exits= short_exits,\n",
    "                size = sizing,\n",
    "                size_type= \"amount\",\n",
    "                fees= 0.001,\n",
    "                slippage=0.001)  # This line integrates your sizing logic\n",
    "\n",
    "            return {\"pf\": pf, \"Price\": {\"X1\" : X1, \"X2\": X2}, \"positions\" : {\"entries\" : short_entries.loc[:, S2], \"exits\": short_exits.loc[:, S2]}, \"sizing\": sizing} \n",
    "        \n",
    "        \n",
    "\n",
    "    def f_pre_backtest(dict_output : dict, window : int, entry_threshold: float, exit_threshold: float) -> dict:\n",
    "            \n",
    "        # 2 Normaliser nos time series Mt pour chaque pair à l'aide vbt indicator factory\n",
    "        \n",
    "        Mt_inSample = dict_output[\"Mt\"][\"inSample\"]\n",
    "        strategy_inSample = vbt_strategyPCI.f_get_Mt_normalize(Mt = Mt_inSample, window = window)\n",
    "        \n",
    "        Mt_outSample = dict_output[\"Mt\"][\"outSample\"]\n",
    "        strategy_outSample = vbt_strategyPCI.f_get_Mt_normalize(Mt = Mt_outSample, window = window)\n",
    "        \n",
    "        \n",
    "        clean_upper_crossed_inSample, clean_lower_crossed_inSample = vbt_strategyPCI.f_get_signals(strategy= strategy_inSample , entry_threshold = entry_threshold, exit_threshold = exit_threshold)\n",
    "        \n",
    "        clean_upper_crossed_outSample, clean_lower_crossed_outSample = vbt_strategyPCI.f_get_signals(strategy= strategy_outSample , entry_threshold = entry_threshold, exit_threshold = exit_threshold)\n",
    "\n",
    "        return {\"inSample\": {\"clean_upper_crossed\" : clean_upper_crossed_inSample, \"clean_lower_crossed\" : clean_lower_crossed_inSample, \"Mt\": strategy_inSample},\n",
    "                \"outSample\" : {\"clean_upper_crossed\" : clean_upper_crossed_outSample, \"clean_lower_crossed\" : clean_lower_crossed_outSample, \"Mt\": strategy_outSample}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def main(path: str, year: int) -> None:\n",
    "# Load adjPrice for all tickers\n",
    "year  = 2008\n",
    "path = \"/Users/sebastiencaron/Desktop/PCI-Project/strategyV1/data/\"\n",
    "filepath1 = f\"{path}adjPrice_nasdaq100.pkl\"\n",
    "adjPrice = Definition.f_load_adjPrice(filepath1)\n",
    "\n",
    "\n",
    "# Load the CSV file : Compustat daily consituent\n",
    "filepath2 = f\"{path}NasdaqConstituent.csv\"\n",
    "object_nasdaq = PCIData.DataCollection(filepath2)\n",
    "# Liste des consituents pour une certaine année\n",
    "\n",
    "consituent2008 = object_nasdaq.list_nasdaq_constituents_by_year(year)\n",
    "\n",
    "# Get price series for list of tickers\n",
    "df_2008 = Definition.f_selectTicker(consituent2008, adjPrice)\n",
    "\n",
    "# Split In and Out sample\n",
    "df_2008_in_sample, df_2008_out_sample = Definition.f_insample_outsample(df_2008, year)\n",
    "\n",
    "# Get the In and Out sample \n",
    "df_2008_inout = pd.concat([df_2008_in_sample, df_2008_out_sample])\n",
    "\n",
    "# Save in stock prices dictionnary\n",
    "stockPrices = {\"inSample\": df_2008_in_sample,\n",
    "            \"outSample\": df_2008_out_sample}\n",
    "\n",
    "# Get the list of consituent of price that we have\n",
    "consituentList = df_2008_in_sample.columns.tolist()\n",
    "\n",
    "# Generate all possible permutations of pairs (including reversed pairs)\n",
    "pairData = Definition.pairDatabase(consituentList=consituentList)\n",
    "\n",
    "pairName = pairData[\"pair\"]\n",
    "inSample = stockPrices[\"inSample\"]\n",
    "pairTest  = pairName[0:1000]\n",
    "\n",
    "# Takes a lot of time to run\n",
    "# elligibilityData, estimationData = Computation.f_compute_estimate(pairTest, inSample, pairData)\n",
    "with open('data.pickle', 'rb') as f:\n",
    "    elligibilityData, estimationData = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "### To add in an other category\n",
    "elligibility_sorted_data, estimatorData =  Computation.f_create_dataframe(elligibilityData, estimationData, pairTest)\n",
    "\n",
    "# Number of pair we keep\n",
    "n_keep = 10\n",
    "\n",
    "pairListSelected = Computation.f_pairSelected(elligibility_sorted_data, n_keep=n_keep)\n",
    "\n",
    "stockList = Computation.get_stock_list(pairData, pairListSelected)\n",
    "\n",
    "\n",
    "### VECTOR BT PART\n",
    "\n",
    "#1) Sortir les informations : Mt, hedgeS1, price of inSample et outSample\n",
    "dict_data = vbt_strategyPCI.f_pre_vectorBT(pairListSelected, estimatorData, pairData, stockPrices[\"inSample\"], stockPrices[\"outSample\"])\n",
    "\n",
    "\"\"\" #2) Normaliser nos time series Mt pour chaque pair à l'aide vbt indicator factory\n",
    "window = 30\n",
    "Mt_inSample = dict_data[\"Mt\"][\"inSample\"]\n",
    "strategy = f_get_Mt_normalize(Mt = Mt_inSample, window = window)\n",
    "\n",
    "#3) Get signals : entry and exit \n",
    "clean_upper_crossed, clean_lower_crossed = f_get_signals(strategy= strategy , entry_threshold = 0.1, exit_threshold = -0.1)\n",
    "\"\"\"\n",
    "\n",
    "window = 30\n",
    "entry_threshold = 0.1\n",
    "exit_threshold = -0.1\n",
    "dict_pre_backtest = vbt_strategyPCI.f_pre_backtest(dict_output = dict_data, window  = window, entry_threshold = entry_threshold, exit_threshold = exit_threshold)\n",
    "\n",
    "\n",
    "#4) Get portfolio backtest\n",
    "type_back = \"inSample\"\n",
    "hedgeS1_inSample = dict_data[\"hedgeS1\"][type_back]\n",
    "price = dict_data[\"Price\"][type_back]\n",
    "pairBacktested = pairListSelected[0]\n",
    "pf_dict = vbt_strategyPCI.f_backtest(dict_pre_backtest = dict_pre_backtest,type_back = type_back, hedgeS1 = hedgeS1_inSample, price = price, pairBacktested = pairBacktested, pairData=pairData)\n",
    "pf = pf_dict[\"pf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning: Metric 'sharpe_ratio' requires frequency to be set\n",
      "  warnings.warn(warning_message)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning: Metric 'calmar_ratio' requires frequency to be set\n",
      "  warnings.warn(warning_message)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning: Metric 'omega_ratio' requires frequency to be set\n",
      "  warnings.warn(warning_message)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning: Metric 'sortino_ratio' requires frequency to be set\n",
      "  warnings.warn(warning_message)\n",
      "/var/folders/hv/1_92kz110w572ncggsb_c81c0000gp/T/ipykernel_758/3705677322.py:1: UserWarning: Object has multiple columns. Aggregating using <function mean at 0x10b4b7eb0>. Pass column to select a single column/group.\n",
      "  pf.stats()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Start                         2004-01-02 00:00:00\n",
       "End                           2007-12-31 00:00:00\n",
       "Period                                       1006\n",
       "Start Value                                 100.0\n",
       "End Value                              100.010916\n",
       "Total Return [%]                         0.010916\n",
       "Benchmark Return [%]                   240.959293\n",
       "Max Gross Exposure [%]                   0.040807\n",
       "Total Fees Paid                          0.008645\n",
       "Max Drawdown [%]                         0.378481\n",
       "Max Drawdown Duration                       508.0\n",
       "Total Trades                             7.090909\n",
       "Total Closed Trades                      6.909091\n",
       "Total Open Trades                        0.181818\n",
       "Open Trade PnL                           -0.00224\n",
       "Win Rate [%]                            60.526316\n",
       "Best Trade [%]                          37.643311\n",
       "Worst Trade [%]                        -20.212666\n",
       "Avg Winning Trade [%]                    7.720681\n",
       "Avg Losing Trade [%]                    -5.051475\n",
       "Avg Winning Trade Duration               13.19697\n",
       "Avg Losing Trade Duration               16.348214\n",
       "Profit Factor                            3.210276\n",
       "Expectancy                               0.001904\n",
       "Name: agg_func_mean, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/Users/sebastiencaron/Desktop/PCI-Project/strategyV1/data/\"\n",
    "# Mt, price_tmp = main(path= path, year= 2008)\n",
    "price_tmp = stockPrices[\"inSample\"][stockList]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector BT pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import vectorbt as vbt \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Sortir les informations : Mt, hedgeS1, price of inSample et outSample\n",
    "dict_data = f_pre_vectorBT(pairListSelected, estimatorData, pairData, stockPrices[\"inSample\"], stockPrices[\"outSample\"])\n",
    "\n",
    "\"\"\" #2) Normaliser nos time series Mt pour chaque pair à l'aide vbt indicator factory\n",
    "window = 30\n",
    "Mt_inSample = dict_data[\"Mt\"][\"inSample\"]\n",
    "strategy = f_get_Mt_normalize(Mt = Mt_inSample, window = window)\n",
    "\n",
    "#3) Get signals : entry and exit \n",
    "clean_upper_crossed, clean_lower_crossed = f_get_signals(strategy= strategy , entry_threshold = 0.1, exit_threshold = -0.1)\n",
    "\"\"\"\n",
    "\n",
    "window = 30\n",
    "entry_threshold = 0.1\n",
    "exit_threshold = -0.1\n",
    "dict_pre_backtest = f_pre_backtest(dict_output = dict_data, window  = window, entry_threshold = entry_threshold, exit_threshold = exit_threshold)\n",
    "\n",
    "\n",
    "#4) Get portfolio backtest\n",
    "type_back = \"inSample\"\n",
    "hedgeS1_inSample = dict_data[\"hedgeS1\"][type_back]\n",
    "price = dict_data[\"Price\"][type_back]\n",
    "pairBacktested = pairListSelected[0]\n",
    "pf= f_backtest(dict_pre_backtest = dict_pre_backtest,type_back = type_back, hedgeS1 = hedgeS1_inSample, price = price, pairBacktested = pairBacktested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning: Metric 'sharpe_ratio' requires frequency to be set\n",
      "  warnings.warn(warning_message)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning: Metric 'calmar_ratio' requires frequency to be set\n",
      "  warnings.warn(warning_message)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning: Metric 'omega_ratio' requires frequency to be set\n",
      "  warnings.warn(warning_message)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/vectorbt/generic/stats_builder.py:396: UserWarning: Metric 'sortino_ratio' requires frequency to be set\n",
      "  warnings.warn(warning_message)\n",
      "/var/folders/hv/1_92kz110w572ncggsb_c81c0000gp/T/ipykernel_758/3705677322.py:1: UserWarning: Object has multiple columns. Aggregating using <function mean at 0x10b4b7eb0>. Pass column to select a single column/group.\n",
      "  pf.stats()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Start                         2004-01-02 00:00:00\n",
       "End                           2007-12-31 00:00:00\n",
       "Period                                       1006\n",
       "Start Value                                 100.0\n",
       "End Value                              100.011643\n",
       "Total Return [%]                         0.011643\n",
       "Benchmark Return [%]                    12.066851\n",
       "Max Gross Exposure [%]                   0.021623\n",
       "Total Fees Paid                          0.012882\n",
       "Max Drawdown [%]                          0.09114\n",
       "Max Drawdown Duration                       364.5\n",
       "Total Trades                                 11.5\n",
       "Total Closed Trades                          11.5\n",
       "Total Open Trades                             0.0\n",
       "Open Trade PnL                                0.0\n",
       "Win Rate [%]                            63.043478\n",
       "Best Trade [%]                           9.194193\n",
       "Worst Trade [%]                        -12.388993\n",
       "Avg Winning Trade [%]                    2.414652\n",
       "Avg Losing Trade [%]                    -3.107956\n",
       "Avg Winning Trade Duration               8.749068\n",
       "Avg Losing Trade Duration               18.521739\n",
       "Profit Factor                            1.168545\n",
       "Expectancy                               0.001012\n",
       "Name: agg_func_mean, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Order Id Column  Timestamp      Size     Price      Fees  Side\n",
      "0           0    ADP 2004-02-27  0.023557  42.40755  0.000999  Sell\n",
      "1           1    ADP 2004-03-11  0.023557  42.08204  0.000991   Buy\n",
      "2           2    ADP 2004-04-02  0.022599  44.20575  0.000999  Sell\n",
      "3           3    ADP 2004-05-19  0.022599  44.50446  0.001006   Buy\n",
      "4           4    ADP 2004-08-04  0.024231  41.22873  0.000999  Sell\n",
      "..        ...    ...        ...       ...       ...       ...   ...\n",
      "179       179   AABA 2007-10-24  0.003771  30.64932  0.000116  Sell\n",
      "180       180   AABA 2007-10-30  0.003648  30.86083  0.000113   Buy\n",
      "181       181   AABA 2007-11-09  0.003648  25.76421  0.000094  Sell\n",
      "182       182   AABA 2007-11-14  0.003742  25.09507  0.000094   Buy\n",
      "183       183   AABA 2007-11-15  0.003742  25.39458  0.000095  Sell\n",
      "\n",
      "[184 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Si on est capable d'aggréé le log de trade, on va être capable d'avoir la performance de la stratégie pour plusieurs titres.\n",
    "\n",
    "print(pf.orders.records_readable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer un streamlite pour sélectionner la pair et voir le backtest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_fill = price_tmp.copy()\n",
    "positions_fill.iloc[:,:] = False\n",
    "\n",
    "sizing = positions_fill\n",
    "\n",
    "long_entries = positions_fill\n",
    "short_entries = positions_fill\n",
    "short_exits = positions_fill\n",
    "long_exits = positions_fill\n",
    "\n",
    "short_entries.loc[clean_upper_crossed[pair].values, S2] = True\n",
    "long_entries.loc[clean_upper_crossed[pair].values, S1] = True\n",
    "\n",
    "short_exits.loc[clean_lower_crossed[pair].values, S2] = True\n",
    "long_exits.loc[clean_lower_crossed[pair].values, S1] = True\n",
    "\n",
    "col = [t[:] for t in clean_upper_crossed.columns]\n",
    "\n",
    "# Commencer avec le vecteur de position et après décider si le stock va long ou short, car sinon il peut avoir des mixed signals. \n",
    "# Trouver l'array des hedge ratios pour chq pair\n",
    "\n",
    "for pair in col:\n",
    "\n",
    "     # Les tickers\n",
    "     S1, S2 = Computation.get_stock1_stock2(pair, pairData)\n",
    "     \n",
    "     # Les prix\n",
    "     X1, X2 = Computation.get_stock_price(price_tmp, [S1, S2])\n",
    "     \n",
    "     s1_sizing = hedgeS1.loc[pair]\n",
    "     \n",
    "     s2_sizing = 1 / X2\n",
    "     \n",
    "     sizing.loc[clean_upper_crossed[pair].values, S2] = - 1 * s2_sizing[clean_upper_crossed[pair]]\n",
    "     \n",
    "     sizing.loc[clean_upper_crossed[pair].values, S1] = s1_sizing.loc[clean_upper_crossed[pair].values]\n",
    "     \n",
    "     short_entries.loc[clean_upper_crossed[pair].values, S2] = True\n",
    "     long_entries.loc[clean_upper_crossed[pair].values, S1] = True\n",
    "\n",
    "     short_exits.loc[clean_lower_crossed[pair].values, S2] = True\n",
    "     long_exits.loc[clean_lower_crossed[pair].values, S1] = True\n",
    "     \n",
    "     \n",
    "\n",
    "\n",
    "     \n",
    "\"\"\"\n",
    "exits = long_exits,\n",
    "short_entries = short_entries,\n",
    "short_exits= short_exits\n",
    "\"\"\"\n",
    "\n",
    "pf = vbt.Portfolio.from_signals(\n",
    "     price_tmp,\n",
    "     entries = long_entries,\n",
    "     exits = long_exits,\n",
    "     short_entries = short_entries,\n",
    "     short_exits= short_exits\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
