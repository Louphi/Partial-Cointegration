{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les librairies (À cleaner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "import warnings\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Nasdaq Constituent\n",
    "Objectif : Lister les constituents du Nasdaq pour une certaine date ou interval de temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nasdaq_Consituent :\n",
    "    \"\"\"\n",
    "        Define the list of nasdaq consituent for a certain year or even day (for futur work)\n",
    "        Data : From WRDS -> Compustats and 1990 to 2023\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path)-> None:\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        \n",
    "    def list_nasdaq_constituents_by_year(self, year: int = 2023) -> list:\n",
    "        \"\"\"\n",
    "        List all NASDAQ constituents for a given year based on the dataset, with cleaned ticker symbols.\n",
    "\n",
    "        Parameters:\n",
    "        data (DataFrame): The DataFrame containing NASDAQ constituents data.\n",
    "        year (int): The year for which to list the constituents.\n",
    "\n",
    "        Returns:\n",
    "        list: A list of tickers that were NASDAQ constituents in the given year, with no suffixes.\n",
    "        \"\"\"\n",
    "        # Convert 'from' and 'thru' columns to datetime\n",
    "        self.data['from'] = pd.to_datetime(self.data['from'])\n",
    "        self.data['thru'] = pd.to_datetime(self.data['thru'])\n",
    "\n",
    "        # Filter data for the specified year\n",
    "        constituents = self.data[(self.data['from'].dt.year <= year) & (self.data['thru'].isna() | (self.data['thru'].dt.year >= year))]\n",
    "\n",
    "        # Clean the ticker symbols and get the unique list\n",
    "        tickers = constituents['co_tic'].unique().tolist()\n",
    "        return tickers\n",
    "    \n",
    "\n",
    "    def create_nasdaq_constituents_daily(self) -> list:\n",
    "        \"\"\"\n",
    "        Create a dictionary where each key is a date and the value is a list of tickers that were NASDAQ 100 constituents on that date.\n",
    "\n",
    "        Parameters:\n",
    "        data (DataFrame): The DataFrame containing NASDAQ constituents data.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary with dates as keys and lists of tickers as values.\n",
    "        \"\"\"\n",
    "        # Convert 'from' and 'thru' columns to datetime\n",
    "        self.data['from'] = pd.to_datetime(self.data['from'])\n",
    "        self.data['thru'] = pd.to_datetime(self.data['thru']).fillna(pd.Timestamp('today'))\n",
    "\n",
    "        # Initialize an empty dictionary\n",
    "        constituents_dict = {}\n",
    "\n",
    "        # Iterate over each row in the DataFrame\n",
    "        for index, row in self.data.iterrows():\n",
    "            # Generate a date range for each row\n",
    "            date_range = pd.date_range(start=row['from'], end=row['thru'])\n",
    "\n",
    "            for date in date_range:\n",
    "                # Convert date to string format for dictionary key\n",
    "                date_str = date.strftime('%Y-%m-%d')\n",
    "                \n",
    "                # Add the ticker to the corresponding date in the dictionary\n",
    "                if date_str in constituents_dict:\n",
    "                    constituents_dict[date_str].add(row['co_tic'])\n",
    "                else:\n",
    "                    constituents_dict[date_str] = {row['co_tic']}\n",
    "\n",
    "        # Convert sets to lists for each date\n",
    "        for date in constituents_dict:\n",
    "            constituents_dict[date] = list(constituents_dict[date])\n",
    "\n",
    "        return constituents_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions à classer ou ajouter dans une classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_load_adjPrice(filepath) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Load pandas dataframe of the price series\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Filepath to the adjPrice_nasdaq100.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        adjPrice = pd.read_pickle(filepath)\n",
    "        adjPrice.index = adjPrice[\"date\"]\n",
    "    except Exception as e:\n",
    "        print(\"Error : {e}\")\n",
    "        adjPrice = None\n",
    "    return adjPrice\n",
    "\n",
    "\n",
    "def f_selectTicker(tickerList, adjPrice)-> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Select the columns that are in tickerList\n",
    "\n",
    "    Args:\n",
    "        tickerList (list): List of all tickers to take into considerations\n",
    "        adjPrice (pd.DataFrame): Adjusted price of all tickers\n",
    "    \"\"\"\n",
    "    \n",
    "    return adjPrice.loc[:, adjPrice.columns.isin(tickerList)]\n",
    "\n",
    "def f_insample_outsample(data, year):\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    # Select in-sample data: data from the four years prior to the specified year.\n",
    "    # It checks if the year in the data is less than the specified year\n",
    "    # and greater than or equal to four years before the specified year.\n",
    "    in_sample = data[(data.index.year < year) & (data.index.year >= year - 4)]\n",
    "    \n",
    "    # Select out-sample data: data from the specified year but only for the first six months.\n",
    "    # It checks if the year in the data is exactly the specified year and\n",
    "    # if the month is less than or equal to 6 (January to June).\n",
    "    out_sample = data[(data.index.year == year) & (data.index.month <= 6)]\n",
    "\n",
    "    # Return both the in-sample and out-sample datasets.\n",
    "    return in_sample, out_sample\n",
    "\n",
    "\n",
    "def pairDatabase(consituentList : list) -> pd.DataFrame:\n",
    "    \n",
    "    combinations = [(a, b) for a, b in itertools.permutations(consituentList, 2)]\n",
    "\n",
    "    # Create a DataFrame\n",
    "    pairDatabase = pd.DataFrame(combinations, columns=[\"Stock 1\", \"Stock 2\"])\n",
    "    pairDatabase[\"pair\"] = pairDatabase[\"Stock 1\"] + \"/\" + pairDatabase[\"Stock 2\"]\n",
    "    \n",
    "    return pairDatabase\n",
    "\n",
    "\n",
    "\n",
    "# Find Parameters for each pair\n",
    "def f_get_elligibility(Stock1, Stock2, inSample):\n",
    "    def Rsquare(sigmaM, sigmaR, rho):\n",
    "        return 2 * (sigmaM ** 2) / (2 * (sigmaM ** 2) + (1+rho) * sigmaR ** 2)\n",
    "        \n",
    "    X = inSample[[Stock1, Stock2]].dropna()\n",
    "\n",
    "    X1 = X[Stock1]\n",
    "    X2 = X[Stock2]\n",
    "\n",
    "    coint = partial_cointegration(X1, X2)\n",
    "    alpha_hat, beta_hat, rho_hat, sigma_M_hat, sigma_R_hat, ll_model, ll_randomWalk, W_hat = coint.fit_pci()\n",
    "    # M_hat, R_hat, _ = coint.kalman_estimate(W_hat, rho_hat, sigma_M_hat, sigma_R_hat)\n",
    "\n",
    "    Rsquare_res = Rsquare(sigma_M_hat, sigma_R_hat, rho_hat)\n",
    "    ll_ratio = ll_randomWalk - ll_model\n",
    "    # print([ll_randomWalk, ll_model])\n",
    "    \n",
    "    return {\"elligibility\":[rho_hat, Rsquare_res, ll_ratio],\n",
    "            \"estimators\" :[alpha_hat, beta_hat, rho_hat, sigma_M_hat, sigma_R_hat]}\n",
    "    \n",
    "    \n",
    "def rolling_z_score(series, window):\n",
    "    \"\"\"\n",
    "    Calculate the rolling Z-score of a given time series.\n",
    "    \n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    window (int): The size of the rolling window.\n",
    "    \n",
    "    Returns:\n",
    "    pd.Series: A series of rolling Z-scores.\n",
    "    \"\"\"\n",
    "     # Convert numpy array to pandas Series if necessary\n",
    "    # Convert numpy array to pandas Series if necessary\n",
    "  # Initialize an array to hold the rolling Z-scores\n",
    "    z_scores = np.full(series.shape, np.nan)\n",
    "\n",
    "    # Loop through the series to calculate the rolling mean and standard deviation\n",
    "    for i in range(window - 1, len(series)):\n",
    "        window_values = series[i - window + 1 : i+1]\n",
    "        #print([window_values[-1], series[i]])\n",
    "        window_mean = np.mean(window_values)\n",
    "        window_std = np.std(window_values, ddof=1)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if window_std != 0:\n",
    "            z_scores[i] = (series[i] - window_mean) / window_std\n",
    "\n",
    "    return z_scores\n",
    "\n",
    "\n",
    "def f_compute_estimate(pairName: pd.DataFrame, inSample: pd.DataFrame)-> tuple :\n",
    "    N = len(pairName)\n",
    "    elligibilityData = np.zeros((N, 3))\n",
    "    estimationData = np.zeros((N, 5))\n",
    "    \n",
    "    for pair in tqdm(N, desc=\"Processing Pairs\"):\n",
    "        try:\n",
    "            pairName_i = pairName[pair]\n",
    "            # print(f\"Doing : {pairName_i}\")\n",
    "            Stock1 = pairData[\"Stock 1\"][pair]\n",
    "            Stock2 = pairData[\"Stock 2\"][pair]\n",
    "            \n",
    "            dict_res = f_get_elligibility(Stock1, Stock2, inSample)\n",
    "            elligibilityData[pair] = dict_res[\"elligibility\"]\n",
    "            # latentVariablesData[pairName[pair]] = dict_res[\"latentVariables\"]\n",
    "            estimationData[pair] = dict_res[\"estimators\"]\n",
    "            #print(f\"{pairName_i}: Done\")\n",
    "        except Exception as e: \n",
    "            print(f\"Error with {pairName_i} and {e}\")\n",
    "        # saving files in pickle format \n",
    "        \n",
    "    with open('elligibilityData.pkl', 'wb') as file:\n",
    "        pickle.dump(elligibilityData, file)\n",
    "\n",
    "    with open('estimationData.pkl', 'wb') as file:\n",
    "        pickle.dump(estimationData, file)\n",
    "            \n",
    "    return elligibilityData, estimationData\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions pour la cointégration partielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class partial_cointegration:\n",
    "    \n",
    "    def __init__(self, X1, X2):\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "\n",
    "    def fit_pci(self, tol=0.001):\n",
    "        '''\n",
    "        Fit partial cointegrated model to time series X1 and X2 such that:\n",
    "            - X_2,t = alpha + beta * X_1,t + W_t\n",
    "            - W_t = M_t + R_t\n",
    "            - M_t = rho * M_t-1 + eps(M_t)\n",
    "            - R_t = R_t-1 + eps(R_t)\n",
    "            - eps(M_t) ∼ NID(0, sigma_M)\n",
    "            - eps(R_t) ∼ NID(0, sigma_R)\n",
    "\n",
    "        Parameters:\n",
    "        X1 (numpy.ndarray): time series\n",
    "        X2 (numpy.ndarray): time series, supposedly partially cointegrated with X1\n",
    "\n",
    "        Returns:\n",
    "        alpha (float): estimated value for alpha\n",
    "        beta (float): estimated value for beta\n",
    "        rho (float): estimated AR(1) coefficient / mean reversion coefficient.\n",
    "        sigma_M (float): estimated white noise variance of mean reverting component .\n",
    "        sigma_R (float): estimated white noise variance of random walk component.\n",
    "        '''\n",
    "\n",
    "        # calculate initial guess for beta\n",
    "        results = self.fit_ols_on_diff()\n",
    "        beta_i = results\n",
    "\n",
    "        # calculate initial guess for alpha\n",
    "        alpha_i = self.X2[0] - beta_i * self.X1[0]\n",
    "\n",
    "        # calculate residuals W and initial guesses for rho, sigma_M, sigma_R\n",
    "        W = self.X2 - alpha_i - beta_i * self.X1\n",
    "\n",
    "        params_i = self.fit_mle(W)\n",
    "\n",
    "        rho, sigma_M, sigma_R = params_i\n",
    "        \n",
    "        # perform optimization depending on the mode (Complete Model)\n",
    "        x_i = (alpha_i, beta_i, rho, sigma_M, sigma_R)  # initial guess\n",
    "        res = opt.minimize(self.f_to_min_pci, x_i, args=(self.X1, self.X2), tol=tol)\n",
    "        alpha, beta, rho, sigma_M, sigma_R = res.x\n",
    "        ll_model = -res.fun\n",
    "        \n",
    "        \n",
    "        # perform optimization for the random Walk H0)\n",
    "        # perform optimization depending on the mode (Complete Model)\n",
    "        x_i = (alpha_i, beta_i, sigma_R)  # initial guess\n",
    "        res = opt.minimize(self.f_to_min_pci, x_i, args=(self.X1, self.X2), tol=tol)\n",
    "        ll_randomWalk = -res.fun\n",
    "        \n",
    "        W = self.X2 - alpha - beta * self.X1\n",
    "\n",
    "        return alpha, beta, rho, sigma_M, sigma_R, ll_model, ll_randomWalk, W\n",
    "    \n",
    "\n",
    "    def fit_mle(self, W, tol=0.001):\n",
    "        '''\n",
    "        fit model using Maximum Likelihood estimation. This is used on the residuals W_t of the linear regression of X1 on X2.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy.ndarray): A partial suspected autoregressive time series\n",
    "\n",
    "        Returns:\n",
    "        rho (float): estimated value of rho\n",
    "        sigma_M (float): estimated value of sigma_M\n",
    "        sigma_R (float): estimated value of sigma_R\n",
    "        '''\n",
    "\n",
    "        estimates = []\n",
    "        lls = []\n",
    "\n",
    "        # distribution for random guesses\n",
    "        rnd_rho = stats.uniform(loc=-1, scale=2)\n",
    "\n",
    "        # get initial guesses using lagged variances\n",
    "        x_i = self.lagvar_estimate_par(W)\n",
    "\n",
    "        # set boundaries\n",
    "        bounds = ((-1, 1), (0, np.inf), (0, np.inf))\n",
    "\n",
    "        # function to generate random initial values\n",
    "        std = np.std(np.diff(W))\n",
    "        rnd_sigma = stats.norm(loc=std, scale=std / 2)\n",
    "\n",
    "        def gen_x0():\n",
    "            return rnd_rho.rvs(), rnd_sigma.rvs(), rnd_sigma.rvs()\n",
    "\n",
    "        # minimize\n",
    "        res = opt.minimize(self.f_to_min_par , x0=(x_i), args=(W), bounds=bounds, tol=tol)\n",
    "\n",
    "        if res.success:\n",
    "            estimates.append(res.x)\n",
    "            lls.append(-res.fun)  # save log-likelihood\n",
    "\n",
    "        # repeat minimization with different (random) initial values\n",
    "        n_att = 0\n",
    "        while len(lls) < 10 and n_att < 100:\n",
    "            n_att += 1\n",
    "            x0 = gen_x0()\n",
    "            res = opt.minimize(self.f_to_min_par, x0=(x0), args=(W), bounds=bounds, tol=tol)\n",
    "            if res.success:\n",
    "                estimates.append(res.x)\n",
    "                lls.append(-res.fun)  # save log-likelihood\n",
    "\n",
    "        try:\n",
    "            argmax = np.argmax(lls)  # index of the biggest likelihood\n",
    "            return estimates[argmax]\n",
    "        except:\n",
    "            # print('Estimation failed!')\n",
    "            return len(x0) * [np.nan]  # return nans\n",
    "        \n",
    "        \n",
    "\n",
    "    def lagvar_estimate_par(self, W):\n",
    "        '''\n",
    "        estimate parameters of partial AR model using lagged variances. used for inital estimation of parameters\n",
    "\n",
    "        Parameters\n",
    "        X (numpy.ndarray): A partial autoregressive time series\n",
    "\n",
    "        Returns:\n",
    "        rho_lv (float): estimated value for rho\n",
    "        sigma_M_lv (float): estimated value for sigma_M\n",
    "        sigma_R_lv (float): estimated value for sigma_R\n",
    "        '''\n",
    "\n",
    "        # calculate lagged variances\n",
    "        v1 = np.var(W[1:] - W[:-1])\n",
    "        v2 = np.var(W[2:] - W[:-2])\n",
    "        v3 = np.var(W[3:] - W[:-3])\n",
    "\n",
    "        # rho\n",
    "        rho_lv = -(v1 - 2 * v2 + v3) / (2 * v1 - v2)\n",
    "\n",
    "        # sigma_M\n",
    "        if (rho_lv + 1) / (rho_lv - 1) * (v2 - 2 * v1) > 0:\n",
    "            sigma_M_lv = np.sqrt(1 / 2 * (rho_lv + 1) / (rho_lv - 1) * (v2 - 2 * v1))\n",
    "        else:\n",
    "            sigma_M_lv = 0\n",
    "\n",
    "        # sigma_R\n",
    "        if v2 > 2 * sigma_M_lv ** 2:\n",
    "            sigma_R_lv = np.sqrt(1 / 2 * (v2 - 2 * sigma_M_lv ** 2))\n",
    "        else:\n",
    "            sigma_R_lv = 0\n",
    "\n",
    "        return rho_lv, sigma_M_lv, sigma_R_lv\n",
    "\n",
    "\n",
    "    def fit_ols_on_diff(self):\n",
    "        '''\n",
    "        Fits an OLS model on the first differences of time series X1 and X2\n",
    "\n",
    "        Parameters:\n",
    "        X1 (numpy.ndarray): A time-series\n",
    "        X2 (numpy.ndarray): A time-series\n",
    "\n",
    "        Returns:\n",
    "        results.params[0]: returns the Beta value of our OLS fit\n",
    "        '''\n",
    "        ret_X1 = np.diff(self.X1)\n",
    "        ret_X2 = np.diff(self.X2)\n",
    "\n",
    "        results = sm.OLS(ret_X2, ret_X1).fit()\n",
    "\n",
    "        return results.params[0]\n",
    "\n",
    "    def kalman_estimate(self, W, rho, sigma_M, sigma_R):\n",
    "        '''\n",
    "        Calculate estimates of mean-reverting and random walk components.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy.ndarray): A partial autoregressive time series\n",
    "        rho (float): AR(1) coefficient / mean reversion coefficient.\n",
    "        sigma_M (float): white noise variance of mean reverting component .\n",
    "        sigma_R (float): white noise variance of random walk component.\n",
    "\n",
    "        Returns:\n",
    "        M (numpy.ndarray): An estimate of the mean reverting component of our time series\n",
    "        R (numpy.ndarray): An estimate of the random walk component of our time series\n",
    "        eps (numpy.ndarray): Prediction errors for each time step\n",
    "        '''\n",
    "\n",
    "        # create arrays for storing both components and prediction errors\n",
    "        M = np.zeros(len(W))\n",
    "        R = np.zeros(len(W))\n",
    "        eps = np.zeros(len(W))\n",
    "\n",
    "        # set initial state\n",
    "        if sigma_R == 0:\n",
    "            M[0] = W[0]\n",
    "            R[0] = 0\n",
    "        else:\n",
    "            M[0] = 0\n",
    "            R[0] = W[0]\n",
    "\n",
    "        # calculate Kalman gain\n",
    "        if sigma_M == 0:\n",
    "            K_M = 0\n",
    "            K_R = 1\n",
    "        elif sigma_R == 0:\n",
    "            K_M = 1\n",
    "            K_R = 0\n",
    "        else:\n",
    "            sqr = np.sqrt((1 + rho) ** 2 * sigma_R ** 2 + 4 * sigma_M ** 2)\n",
    "            K_M = 2 * sigma_M ** 2 / (sigma_R * (sqr + rho * sigma_R + sigma_R) + 2 * sigma_M ** 2)\n",
    "            K_R = 2 * sigma_R / (sqr - rho * sigma_R + sigma_R)\n",
    "\n",
    "        # calculate estimates\n",
    "        for i in range(1, len(W)):\n",
    "            xhat = rho * M[i - 1] + R[i - 1]\n",
    "            eps[i] = W[i] - xhat\n",
    "            M[i] = rho * M[i - 1] + eps[i] * K_M\n",
    "            R[i] = R[i - 1] + eps[i] * K_R\n",
    "\n",
    "        return M, R, eps\n",
    "\n",
    "    def calc_log_like(self, W, rho, sigma_M, sigma_R):\n",
    "        '''\n",
    "        Compute log likelihood function\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy.ndarray): A partial autoregressive time series\n",
    "        rho (float): AR(1) coefficient / mean reversion coefficient.\n",
    "        sigma_M (float): white noise variance of mean reverting component .\n",
    "        sigma_R (float): white noise variance of random walk component.\n",
    "\n",
    "        Returns:\n",
    "        ll (float): Value of the log likelihood, a measure of goodness of fit for our model\n",
    "        '''\n",
    "\n",
    "        N = len(W)\n",
    "        _, _, eps = self.kalman_estimate(W, rho, sigma_M, sigma_R)\n",
    "        ll = -(N - 1) / 2 * np.log(2 * np.pi * (sigma_M ** 2 + sigma_R ** 2)) - 1 / (\n",
    "                2 * (sigma_M ** 2 + sigma_R ** 2)) * np.sum(eps[1:] ** 2)\n",
    "\n",
    "        return ll\n",
    "\n",
    "    def f_to_min_par(self, x_i, W):\n",
    "        rho, sigma_M, sigma_R = x_i\n",
    "        '''\n",
    "        Define the function to minimize for PAR model\n",
    "        '''\n",
    "        return -self.calc_log_like(W, rho, sigma_M, sigma_R)\n",
    "\n",
    "    def f_to_min_pci(self, x_i, X1, X2):\n",
    "        '''\n",
    "        Define function to minimize\n",
    "        '''\n",
    "        if len(x_i) == 5 :\n",
    "            alpha, beta, rho, sigma_M, sigma_R = x_i\n",
    "            \n",
    "        elif len(x_i) == 3:\n",
    "            alpha, beta, sigma_R = x_i\n",
    "            rho = 0\n",
    "            sigma_M = 0\n",
    "        \n",
    "        W = X2 - beta * X1 - alpha\n",
    "        return -self.calc_log_like(W, rho, sigma_M, sigma_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pair / Stock class utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an object for pairData, (include everything we need)\n",
    "\n",
    "#Add Insample and outSample functionalities\n",
    "class Pair_utility:\n",
    "    def __init__(self, pairname: str, pairData: dict, estimatorData: pd.DataFrame, stockPrices: pd.Series) -> None: \n",
    "        \n",
    "        self.pairname = pairname\n",
    "        self.pairData = pairData\n",
    "        self.estimatorData = estimatorData\n",
    "        self.stockPrices = stockPrices  # Save stockPrices as an instance attribute\n",
    "        self.fetch_stockname()\n",
    "\n",
    "        stock2_obj = Stock(self.stock2, self.stockPrices)\n",
    "   \n",
    "        self.inout = stock2_obj.inout\n",
    "        \n",
    "        self.f_get_pre_tradingVBT()\n",
    "\n",
    "    def fetch_stockname(self): \n",
    "        tmp = self.pairData[self.pairData[\"pair\"] == self.pairname]\n",
    "        self.stock1 = tmp['Stock 1'].iloc[0]\n",
    "        self.stock2 = tmp['Stock 2'].iloc[0]\n",
    "\n",
    "    def f_get_pre_tradingVBT(self): \n",
    "        X = self.inout[[self.stock1, self.stock2]].dropna()\n",
    "        X1 = X[self.stock1]\n",
    "        X2 = X[self.stock2]\n",
    "        \n",
    "        estimatorData_input = self.estimatorData.loc[self.pairname]\n",
    "        \n",
    "        alpha_hat, beta_hat, rho_hat, sigma_M_hat, sigma_R_hat = estimatorData_input\n",
    "    \n",
    "        W = X2 - alpha_hat - beta_hat * X1\n",
    "\n",
    "        coint = partial_cointegration(X1, X2)\n",
    "\n",
    "        M_hat, R_hat, _ = coint.kalman_estimate(W, rho_hat, sigma_M_hat, sigma_R_hat)\n",
    "      \n",
    "        M_hat_reindexed = pd.Series(M_hat, index = X.index)\n",
    "        hedge_ratio = beta_hat / X2\n",
    "        \n",
    "        self.Mt = M_hat_reindexed\n",
    "        \n",
    "        self.hedge_ratio = hedge_ratio\n",
    "    \n",
    "\n",
    "class Stock:\n",
    "    def __init__(self, ticker : str, stockPrices : pd.Series) -> None:\n",
    "        self.name = ticker\n",
    "        insample = stockPrices[\"inSample\"]\n",
    "        self.insample = insample[self.name]\n",
    "        \n",
    "        outsample = stockPrices[\"outSample\"]\n",
    "        self.outsample = outsample[self.name]\n",
    "        \n",
    "        self.inout = pd.concat([insample, outsample])\n",
    "        \n",
    "        self.priceALL = self.inout[self.name]\n",
    "    \n",
    "\"\"\"\n",
    "Example:\n",
    "pairname = \"AABA/AAPL\"\n",
    "obj = Pair_utility(\"AABA/AAPL\", pairData, estimatorData, stockPrices)\n",
    "obj.Mt\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairs selected Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selected_Pair:\n",
    "    def __init__(self, pct_condition : float, elligibilityData, conditions : dict, pairData) -> None:\n",
    "        self.pct_condition = pct_condition\n",
    "        self.conditions = conditions\n",
    "        self.elligibility_sorted_data = self.f_sort_elligibility_pair(elligibilityData, pairData)\n",
    "        \n",
    "        try:\n",
    "            self.pairListSelected = self.f_pairSelected()\n",
    "        except Exception as e: \n",
    "            print(f\"Error with selection of pair {e}\")\n",
    "            \n",
    "    def f_sort_elligibility_pair(self, elligibilityData, pairData):\n",
    "        # Unpact conditions\n",
    "        l_bounds_rho, h_bounds_rho = self.conditions[\"rho\"]\n",
    "        l_bounds_rsquare, h_bounds_rsquare = self.conditions[\"Rsquare\"]\n",
    "        l_bounds_ll_ratio, h_bounds_ll_ratio = self.conditions[\"ll_ratio\"]\n",
    "        \n",
    "        elligibilityData = pd.DataFrame(elligibilityData, index = pairData[\"pair\"], columns=[\"rho\", \"Rsquare\", \"ll_ratio\"])\n",
    "        elligibilityData[\"ll_ratio\"].fillna(0)\n",
    "        elligibility_sorted_data = elligibilityData[\n",
    "                                                    (elligibilityData[\"rho\"] > l_bounds_rho) &\n",
    "                                                    (elligibilityData[\"rho\"] < h_bounds_rho) &  \n",
    "                                                    (elligibilityData[\"Rsquare\"] > l_bounds_rsquare) & \n",
    "                                                    (elligibilityData[\"Rsquare\"] < h_bounds_rsquare) &\n",
    "                                                    (elligibilityData[\"ll_ratio\"] > l_bounds_ll_ratio) &\n",
    "                                                    (elligibilityData[\"ll_ratio\"] < h_bounds_ll_ratio)\n",
    "                                                ].sort_values(by=\"ll_ratio\", ascending=True)\n",
    "        \n",
    "        return elligibility_sorted_data\n",
    "    \n",
    "    def f_pairSelected(self):\n",
    "        # Nombre de pair elligible\n",
    "        n_elligible = len(self.elligibility_sorted_data.index)\n",
    "        n = max(1, round(self.pct_condition * n_elligible))\n",
    "        \n",
    "        pairListSelected = self.elligibility_sorted_data.index[:n].to_list()\n",
    "\n",
    "        return pairListSelected\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Example : conditions = {\"rho\": (0.5, 0.9),\n",
    "    \"Rsquare\" : (0.5, 10),\n",
    "    \"ll_ratio\" : (0, 1) }\n",
    "\n",
    "pair_selected_obj = Selected_Pair(0.01, elligibilityData, conditions, pairData)\n",
    "pair_selected_obj.pairListSelected\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code A à Z \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load adjPrice for all tickers\n",
    "filepath1 = r\"/Users/sebastiencaron/Desktop/PCI-Project/strategyV1/data/adjPrice_nasdaq100.pkl\"\n",
    "adjPrice = f_load_adjPrice(filepath1)\n",
    "\n",
    "\n",
    "# Load the CSV file : Compustat daily consituent\n",
    "filepath2 = r'/Users/sebastiencaron/Desktop/PCI-Project/strategyV1/data/NasdaqConstituent.csv'\n",
    "object_nasdaq = Nasdaq_Consituent(filepath2)\n",
    "# Liste des consituents pour une certaine année\n",
    "\n",
    "year = 2008\n",
    "consituent2008 = object_nasdaq.list_nasdaq_constituents_by_year(year)\n",
    "\n",
    "# Get price series for list of tickers\n",
    "df_2008 = f_selectTicker(consituent2008, adjPrice)\n",
    "\n",
    "# Split In and Out sample\n",
    "df_2008_in_sample, df_2008_out_sample = f_insample_outsample(df_2008, year)\n",
    "\n",
    "# Get the In and Out sample \n",
    "df_2008_inout = pd.concat([df_2008_in_sample, df_2008_out_sample])\n",
    "\n",
    "# Save in stock prices dictionnary\n",
    "stockPrices = {\"inSample\": df_2008_in_sample,\n",
    "               \"outSample\": df_2008_out_sample}\n",
    "\n",
    "# Get the list of consituent of price that we have\n",
    "consituentList = df_2008_in_sample.columns.tolist()\n",
    "\n",
    "# Generate all possible permutations of pairs (including reversed pairs)\n",
    "pairData = pairDatabase(consituentList=consituentList)\n",
    "\n",
    "pairName = pairData[\"pair\"]\n",
    "inSample = stockPrices[\"inSample\"]\n",
    "\n",
    "# Takes a lot of time to run\n",
    "# elligibilityData, estimationData = f_compute_estimate(pairName, inSample)\n",
    "\n",
    "elligibilityData = pd.read_pickle(r\"/Users/sebastiencaron/Desktop/PCI-Project/strategyV1/data/elligibilityData.pkl\")\n",
    "\n",
    "estimationData = pd.read_pickle(r\"/Users/sebastiencaron/Desktop/PCI-Project/strategyV1/data/estimationData.pkl\")\n",
    "\n",
    "# Rearrange the dataframe depending of the conditions\n",
    "# elligibility_sorted_data, estimatorData = f_create_dataframe(elligibilityData, estimationData, pairName)\n",
    "\n",
    "estimatorData = pd.DataFrame(estimationData, index = pairName, columns=[\"alpha_hat\", \"beta_hat\", \"rho_hat\", \"sigma_M_hat\", \"sigma_R_hat\"])\n",
    "\n",
    "# Get portolio shares and pre_trading dataframe for every pair\n",
    "# Percentage of elligible pair we select (best to worst)\n",
    "pct_condition = 0.05\n",
    "\n",
    "conditions = {  \"rho\": (0.5, 0.9),\n",
    "                \"Rsquare\" : (0.5, 10),\n",
    "                \"ll_ratio\" : (0, 1) }\n",
    "\n",
    "pair_selected_obj = Selected_Pair(0.01, elligibilityData, conditions, pairData)\n",
    "testPair = pair_selected_obj.pairListSelected[0]\n",
    "\n",
    "\n",
    "obj = Pair_utility(testPair, pairData, estimatorData, stockPrices)\n",
    "S1 = obj.stock1\n",
    "S2 = obj.stock2\n",
    "Mt = obj.Mt\n",
    "hedge_ratio = obj.hedge_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector BT pro advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/sebastiencaron/Desktop/vectorbt.pro\")\n",
    "import vectorbtpro as vbt# Assuming the file name is vectorbtpro.py\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score = vbt.IndicatorFactory(\n",
    "        class_name = \"zscore\",\n",
    "        short_name = \"zs\",\n",
    "        input_names = [\"series\"],\n",
    "        param_names = [\"window\"],\n",
    "        output_names = [\"z_score\"],\n",
    "        ).with_apply_func(rolling_z_score,\n",
    "                           window = 5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the Z-score indicator on the data\n",
    "data = df_2008_inout\n",
    "Mt = obj.Mt\n",
    "hedge_ratio = obj.hedge_ratio\n",
    "z_score_series = z_score.run(Mt)\n",
    "z_scores = z_score_series.z_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fill = data.copy()\n",
    "data_fill.loc[:,:] = False\n",
    "\n",
    "\n",
    "# Define the thresholds\n",
    "entry_threshold = 1  # Example threshold for entry\n",
    "exit_threshold = -0.5   # Example threshold for exit\n",
    "# Generate entry signals (Z-score crosses below the entry threshold)\n",
    "upper_crossed = z_score_series.z_score.vbt.crossed_above(entry_threshold)\n",
    "\n",
    "# Generate exit signals (Z-score crosses above the exit threshold)\n",
    "lower_crossed = z_score_series.z_score.vbt.crossed_below(exit_threshold)\n",
    "\n",
    "clean_upper_crossed, clean_lower_crossed = upper_crossed.vbt.signals.clean(lower_crossed)  \n",
    "series = z_score_series.z_score\n",
    "\n",
    "def plot_z(series, entries, exits):\n",
    "    fig = series.vbt.plot() \n",
    "    entries.vbt.signals.plot_as_entries(series, fig=fig)  \n",
    "    exits.vbt.signals.plot_as_exits(series, fig=fig)  \n",
    "    return fig\n",
    "\n",
    "plot_z(series, clean_upper_crossed, clean_lower_crossed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_entries = data_fill\n",
    "short_entries = data_fill\n",
    "\n",
    "clean_upper_crossed_reindexed = clean_upper_crossed.reindex(short_entries.index, fill_value=False)\n",
    "clean_lower_crossed_reindexed = clean_upper_crossed.reindex(short_entries.index, fill_value=False)\n",
    "\n",
    "short_entries.loc[clean_upper_crossed_reindexed, S2] = True\n",
    "long_entries.loc[clean_upper_crossed_reindexed, S1] = True\n",
    "long_entries.loc[clean_lower_crossed_reindexed, S2] = True\n",
    "short_entries.loc[clean_lower_crossed_reindexed, S1] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = vbt.Portfolio.from_signals(\n",
    "     datatest,\n",
    "     entries=long_entries,\n",
    "     short_entries=short_entries,\n",
    "     size=10,  \n",
    "     size_type= \"Amount\",  \n",
    "     group_by=True,  \n",
    "     cash_sharing=True,\n",
    "     call_seq=\"auto\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
